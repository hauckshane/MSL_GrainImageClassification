---
title: "Data Pre-Processing and Explorations"
author: "Shane Hauck"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Packages
```{r}
library(tidyverse)
```

# Load Data
```{r}
train <- read_csv("train_data.csv")
test_x <- read_csv("test_data_x.csv")

```

# Data Pre-Processing
```{r}
# Check for missing values
sum(is.na(train))
sum(is.na(test_x))

# Check for duplicates
train %>% distinct() %>% nrow()
test_x %>% distinct() %>% nrow()

# Check for outliers
train %>% summary()
test_x %>% summary()

# Check for class imbalance
train %>% group_by(Y) %>% count()

# Check for multicollinearity
library(GGally)
# ggpairs(train)


```


# Split train into train and validation
```{r}
set.seed(123)
train_index <- sample(1:nrow(train), 0.8*nrow(train))
train_data <- train[train_index,]
validation_data <- train[-train_index,]

```

# Create matrix of predictors and response
```{r}
# Training set
X_train <- as.matrix(train_data %>% dplyr::select(-Y))
Y_train <- as.matrix(train_data %>% dplyr::select(Y))

# Validation set
X_val <- as.matrix(validation_data %>% dplyr::select(-Y))
Y_val <- as.matrix(validation_data %>% dplyr::select(Y))

```

# Fit a LASSO model 
```{r}
library(glmnet)

# Fit the model
lasso_model <- cv.glmnet(X_train, Y_train, family = "binomial", alpha = 1)

# Make predictions
Y_pred_lasso <- predict(lasso_model, s = "lambda.min", newx = X_val, type = "response")

# Calculate the accuracy
Y_pred_class_lasso <- ifelse(Y_pred_lasso > 0.5, 1, 0)
mean(Y_pred_class_lasso == Y_val)

# Confusion matrix
table(Y_pred_class_lasso, Y_val)

# Log likelihood loss
log_loss <- -mean(Y_val*log(Y_pred_lasso) + (1-Y_val)*log(1-Y_pred_lasso))
log_loss

# Show the coefficients
coef(lasso_model, s = "lambda.min")


# Bootstrapping
boot_lasso_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- cv.glmnet(X_boot, Y_boot, family = "binomial", alpha = 1)
  boot_pred <- predict(boot_model, s = "lambda.min", newx = X_boot_test, type = "response")
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_lasso_acc <- c(boot_lasso_acc, boot_acc)
}

mean_boot_lasso <- mean(boot_lasso_acc)


```

# Fit a Ridge model
```{r}
# Fit the model
ridge_model <- cv.glmnet(X_train, Y_train, family = "binomial", alpha = 0)

# Make predictions
Y_pred_ridge <- predict(ridge_model, s = "lambda.min", newx = X_val, type = "response")

# Calculate the accuracy
Y_pred_class_ridge <- ifelse(Y_pred_ridge > 0.5, 1, 0)
mean(Y_pred_class_ridge == Y_val)

# Confusion matrix
table(Y_pred_class_ridge, Y_val)

# Log likelihood loss
log_loss <- -mean(Y_val*log(Y_pred_ridge) + (1-Y_val)*log(1-Y_pred_ridge))
log_loss

# Show the coefficients
coef(ridge_model, s = "lambda.min")

# Bootstrapping
boot_ridge_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- cv.glmnet(X_boot, Y_boot, family = "binomial", alpha = 0)
  boot_pred <- predict(boot_model, s = "lambda.min", newx = X_boot_test, type = "response")
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_ridge_acc <- c(boot_ridge_acc, boot_acc)
}

mean_boot_ridge <- mean(boot_ridge_acc)

```

# Fit a Elastic Net model
```{r}
# Fit the model
enet_model <- cv.glmnet(X_train, Y_train, family = "binomial", alpha = 0.5)

# Make predictions
Y_pred_enet <- predict(enet_model, s = "lambda.min", newx = X_val, type = "response")
Y_pred_class_enet <- ifelse(Y_pred_enet > 0.5, 1, 0)

# Calculate the accuracy
mean(Y_pred_class_enet == Y_val)

# Confusion matrix
table(Y_pred_class_enet, Y_val)

# Bootstrapping
boot_enet_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- cv.glmnet(X_boot, Y_boot, family = "binomial", alpha = 0.5)
  boot_pred <- predict(boot_model, s = "lambda.min", newx = X_boot_test, type = "response")
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_enet_acc <- c(boot_enet_acc, boot_acc)
}

mean_boot_enet <- mean(boot_enet_acc)

```

# Fit a Random Forest model
```{r}
library(randomForest)

# Fit the model
rf_model <- randomForest(X_train, Y_train, ntree = 25, mtry = 3)

# Make predictions
Y_pred_rf <- predict(rf_model, X_val, type = "response")

# Calculate the accuracy
Y_pred_class_rf <- ifelse(Y_pred_rf > 0.5, 1, 0)
mean(Y_pred_class_rf == Y_val)

# Confusion matrix  
table(Y_pred_class_rf, Y_val)

# Bootstrapping
boot_rf_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- randomForest(X_boot, Y_boot, ntree = 25, mtry = 3)
  boot_pred <- predict(boot_model, X_boot_test, type = "response")
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_rf_acc <- c(boot_rf_acc, boot_acc)
}

mean_boot_rf <- mean(boot_rf_acc)

```

# Fit a Gradient Boosting model
```{r}
library(xgboost)

# Fit the model
xgb_model <- xgboost(data = X_train, label = Y_train, nrounds = 25, objective = "binary:logistic")

# Make predictions
Y_pred_xgb <- predict(xgb_model, X_val)

# Calculate the accuracy
Y_pred_class_xgb <- ifelse(Y_pred_xgb > 0.5, 1, 0)
mean(Y_pred_class_xgb == Y_val)

# Confusion matrix
table(Y_pred_class_xgb, Y_val)

# Bootstrapping
boot_xgb_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- xgboost(data = X_boot, label = Y_boot, nrounds = 25, objective = "binary:logistic")
  boot_pred <- predict(boot_model, X_boot_test)
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_xgb_acc <- c(boot_xgb_acc, boot_acc)
}

mean_boot_xgb <- mean(boot_xgb_acc)

```

# Fit a K-Nearest Neighbors model
```{r}
library(class)

# Fit the model
knn_model <- knn(X_train, X_val, Y_train, k = 5)

# Calculate the accuracy
mean(knn_model == Y_val)

# Confusion matrix
table(knn_model, Y_val)

# Bootstrapping
boot_knn_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- knn(X_boot, X_boot_test, Y_boot, k = 5)
  boot_acc <- mean(boot_model == Y_boot_test)
  boot_knn_acc <- c(boot_knn_acc, boot_acc)
}

mean_boot_knn <- mean(boot_knn_acc)

```

# Fit a Support Vector Machine model
```{r}
library(e1071)

# Fit the model
svm_model <- svm(X_train, Y_train, kernel = "radial", cost=0.7, gamma=0.01)

# Make predictions
Y_pred_svm <- predict(svm_model, X_val)
Y_pred_class_svm <- ifelse(Y_pred_svm > 0.5, 1, 0)

# Calculate the accuracy
mean(Y_pred_class_svm == Y_val)

# Confusion matrix
table(Y_pred_class_svm, Y_val)

# Bootstrapping
boot_svm_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- svm(X_boot, Y_boot, kernel = "radial", cost=0.7, gamma=0.01)
  boot_pred <- predict(boot_model, X_boot_test)
  boot_class <- ifelse(boot_pred > 0.5, 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_svm_acc <- c(boot_svm_acc, boot_acc)
}

mean_boot_svm <- mean(boot_svm_acc)

```

# Fit a Tree model
```{r}
library(rpart)
# Fit the model
tree_model <- rpart(Y ~ ., data = train_data, method = "class")

# Make predictions
Y_pred_tree <- predict(tree_model, validation_data, type = "class")
Y_pred_class_tree <- ifelse(Y_pred_tree == "1", 1, 0)

# Calculate the accuracy
mean(Y_pred_class_tree == Y_val)

# Confusion matrix
table(Y_pred_class_tree, Y_val)

# Bootstrapping
boot_tree_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- rpart(Y ~ ., data = boot_data, method = "class")
  boot_pred <- predict(boot_model, boot_data_test, type = "class")
  boot_class <- ifelse(boot_pred == "1", 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_tree_acc <- c(boot_tree_acc, boot_acc)
}

mean_boot_tree <- mean(boot_tree_acc)

```

# Fit a Linear Discriminant Analysis model
```{r}
library(MASS)

# Fit the model
lda_model <- lda(Y ~ ., data = train_data)

# Make predictions
Y_pred_lda <- predict(lda_model, validation_data)$class
Y_pred_class_lda <- ifelse(Y_pred_lda == "1", 1, 0)

# Calculate the accuracy
mean(Y_pred_class_lda == Y_val)

# Confusion matrix
table(Y_pred_class_lda, Y_val)

# Bootstrapping
boot_lda_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- lda(Y ~ ., data = boot_data)
  boot_pred <- predict(boot_model, boot_data_test)$class
  boot_class <- ifelse(boot_pred == "1", 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_lda_acc <- c(boot_lda_acc, boot_acc)
}

mean_boot_lda <- mean(boot_lda_acc)

```

# Fit a Quadratic Discriminant Analysis model
```{r}
# Fit the model
qda_model <- qda(Y ~ ., data = train_data)

# Make predictions
Y_pred_qda <- predict(qda_model, validation_data)$class
Y_pred_class_qda <- ifelse(Y_pred_qda == "1", 1, 0)

# Calculate the accuracy
mean(Y_pred_class_qda == Y_val)

# Confusion matrix
table(Y_pred_class_qda, Y_val)

# Bootstrapping
boot_qda_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- qda(Y ~ ., data = boot_data)
  boot_pred <- predict(boot_model, boot_data_test)$class
  boot_class <- ifelse(boot_pred == "1", 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_qda_acc <- c(boot_qda_acc, boot_acc)
}

mean_boot_qda <- mean(boot_qda_acc)

```

# Fit a Naive Bayes model
```{r}
library(e1071)

# Fit the model
nb_model <- naiveBayes(Y ~ ., data = train_data)

# Make predictions
Y_pred_nb <- predict(nb_model, validation_data)
Y_pred_class_nb <- ifelse(Y_pred_nb == "1", 1, 0)

# Calculate the accuracy
mean(Y_pred_class_nb == Y_val)

# Confusion matrix
table(Y_pred_class_nb, Y_val)

# Bootstrapping
boot_nb_acc <- c()
set.seed(123)
for (i in 1:25){
  boot_index <- sample(1:nrow(train), nrow(train), replace = TRUE)
  boot_data <- train[boot_index,]
  boot_data_test <- train[-boot_index,]
  X_boot <- as.matrix(boot_data %>% dplyr::select(-Y))
  Y_boot <- as.matrix(boot_data %>% dplyr::select(Y))
  X_boot_test <- as.matrix(boot_data_test %>% dplyr::select(-Y))
  Y_boot_test <- as.matrix(boot_data_test %>% dplyr::select(Y))
  boot_model <- naiveBayes(Y ~ ., data = boot_data)
  boot_pred <- predict(boot_model, boot_data_test)
  boot_class <- ifelse(boot_pred == "1", 1, 0)
  boot_acc <- mean(boot_class == Y_boot_test)
  boot_nb_acc <- c(boot_nb_acc, boot_acc)
}

mean_boot_nb <- mean(boot_nb_acc)

```



# Report the results for each model
```{r}
# Collect accuracy scores
accuracy_scores <- tibble(
  Method = c("LASSO", "Ridge", "Elastic Net", "Random Forest", "Gradient Boosting", 
             "K-Nearest Neighbors", "Support Vector Machine", "Tree", 
             "Linear Discriminant Analysis", "Quadratic Discriminant Analysis", "Naive Bayes"),
  Accuracy = c(
    mean(Y_pred_class_lasso == Y_val),
    mean(Y_pred_class_ridge == Y_val),
    mean(Y_pred_class_enet == Y_val),
    mean(Y_pred_class_rf == Y_val),
    mean(Y_pred_class_xgb == Y_val),
    mean(knn_model == Y_val),
    mean(Y_pred_class_svm == Y_val),
    mean(Y_pred_class_tree == Y_val),
    mean(Y_pred_class_lda == Y_val),
    mean(Y_pred_class_qda == Y_val),
    mean(Y_pred_class_nb == Y_val)
  )
)

# Print the accuracy scores ordered by accuracy
accuracy_scores %>% arrange(desc(Accuracy))

# Collect bootstrapped accuracy scores
boot_accuracy_scores <- tibble(
  Method = c("LASSO", "Ridge", "Elastic Net", "Random Forest", "Gradient Boosting", 
             "K-Nearest Neighbors", "Support Vector Machine", "Tree", 
             "Linear Discriminant Analysis", "Quadratic Discriminant Analysis", "Naive Bayes"),
  Boot_Accuracy = c(
    mean_boot_lasso,
    mean_boot_ridge,
    mean_boot_enet,
    mean_boot_rf,
    mean_boot_xgb,
    mean_boot_knn,
    mean_boot_svm,
    mean_boot_tree,
    mean_boot_lda,
    mean_boot_qda,
    mean_boot_nb
  )
)

# Print the bootstrapped accuracy scores ordered by accuracy
boot_accuracy_scores %>% arrange(desc(Boot_Accuracy))

```

# Make final predictions with SVM
```{r}
# Train X
X_train <- as.matrix(train %>% dplyr::select(-Y))
# Test X
X_test <- as.matrix(test_x)

# Train Y
Y_train <- as.matrix(train %>% dplyr::select(Y))


# Fit the model
svm_model <- svm(Y ~ ., data = train_data, kernel = "radial", cost=0.7, gamma=0.01)

# Make predictions
Y_pred_svm <- predict(svm_model, X_test)
Y_pred_class_svm <- ifelse(Y_pred_svm > 0.5, 1, 0)

# Make df with columns y.guesses and test.acc
df <- data.frame(y.guesses = Y_pred_class_svm,
                 test.acc = .92,
                 team.name = "Hauck_Strennen")

# Save df
save(df, file = "Hauck_Strennen.RData")
```



